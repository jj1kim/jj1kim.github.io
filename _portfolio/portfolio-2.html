---
title: "Layered Prefill"
excerpt: "An innovative scheduling paradigm for LLM serving that treats transformer layer groups as the primary scheduling unit, improving efficiency and reducing latency.<br/><img src='/images/500x300.png'>"
collection: portfolio
header:
  teaser: layered_prefill.png
---

Layered Prefill introduces a novel approach to LLM inference scheduling by vertically partitioning transformer models into contiguous layer groups. This method enables more efficient resource utilization and significantly reduces time-to-first-token (TTFT) and end-to-end latency. The project showcases deep understanding of transformer architectures and system-level optimization techniques. 
